OLMo项目由AI2发起，作为一个真正的非营利性组织，致力于实现完全开源的目标。这一项目彻底开放了其完整的预训练数据——覆盖三万亿token的Dolma数据集。

## 开源内容概述

OLMo不但提供了其训练代码、模型权重、推理代码、训练指标和完整日志等全部原始数据。这种完整的开放程度让研究人员能够完全复现模型训练过程，深入理解模型的性能表现，并根据自身需要对模型进行微调。

### OLMo框架的组成部分

- **完整的预训练数据**：OLMo项目提供的Dolma数据集，包含三万亿token，为语言模型的预训练提供了丰富的语料。这意味着研究人员不仅可以访问模型本身，还能获得训练这些模型所使用的原始数据，从而更好地理解模型的学习基础，甚至进行再训练或特殊调整。

- **训练代码与模型权重**：OLMo提供了四种不同变体模型的完整权重，每个模型至少经过2万亿token的训练。除了数据外，OLMo还提供了丰富的训练代码、推理代码和日志记录，确保研究人员可以有效复现训练过程及性能分析。

- **评估工具**：项目包括开发过程中使用的多种评估工具和500多个模型检查点，为研究人员在评估和分析OLMo模型时提供支持。

## 模型参数与架构

OLMo提供了不同规模的模型变体：

- **1B（10亿参数）模型**：具有16层，每层2048个隐藏单元，16个注意力头，训练了至少2万亿个令牌。
  
- **7B（70亿参数）模型**：包含32层，每层有4086个隐藏单元，32个注意力头，训练了约2.46万亿个令牌。
  
- **65B（650亿参数）模型**：仍在训练中，计划包含80层，每层8192个隐藏单元，64个注意力头。

这些模型基于Vaswani等（2017年）的解码器仅Transformer架构，并进行了如下改进：

- 不使用偏置项，以提高训练稳定性。
- 采用非参数层归一化。
- 使用SwiGLU激活函数替代ReLU。
- 引入旋转位置嵌入（RoPE）。
- 使用修改版BPE标记器，以减少个人可识别信息（PII）。

## Dolma预训练数据

OLMo使用的Dolma数据集是一个多源、多样性的语料库，包含来自7个不同数据源的5亿文档，旨在促进语言模型预训练的研究，包括网页、代码、社交媒体、STEM论文、书籍和百科资料等。

## 性能评估

OLMo 7B在生成与阅读理解任务（如truthfulQA）中表现与Llama 2相当，但在流行的问答任务（如MMLU或Big-bench Hard）上略显逊色。通过AI2的Paloma与可用检查点，研究分析了模型预测语言能力与模型规模（如训练token数）之间的关系。

👉 [野卡 | 一分钟注册，轻松订阅海外线上服务](https://bit.ly/bewildcard)

项目地址：[OLMo项目](https://allenai.org/olmo)